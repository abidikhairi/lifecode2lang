{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "381d35ee",
   "metadata": {},
   "source": [
    "# Continued Pretraining Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "946b28a8",
   "metadata": {},
   "source": [
    "Continued pretraining is the process of taking a pre-existing language model and further training it on a domain-specific corpus to adapt its knowledge to a particular context.\n",
    "\n",
    "### Notebook Overview\n",
    "\n",
    "In this notebook, we fine-tune SmolLM2 (135M) on SwissProt sequences to enhance its understanding of protein language and improve performance on downstream protein-related tasks.\n",
    "\n",
    "- Model Card: [https://huggingface.co/HuggingFaceTB/SmolLM-135M](HuggingFaceTB/SmolLM-135M)\n",
    "- Dataset Link: [https://huggingface.co/datasets/khairi/uniprot-swissprot](khairi/uniprot-swissprot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3c32c156",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, DataCollatorForLanguageModeling\n",
    "from transformers.trainer import Trainer\n",
    "from transformers.training_args import TrainingArguments, OptimizerNames\n",
    "from peft import get_peft_model, LoraConfig, TaskType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da020eaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment to use wandb logging\n",
    "# !wandb login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc90c3d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment to enable pushing to the HuggingFace Hub\n",
    "# !huggingface-cli login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e9597e7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"HuggingFaceTB/SmolLM2-135M\" # use a smaller model for testing\n",
    "dataset_id = \"khairi/uniprot-swissprot\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f4ad7d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file vocab.json from cache at /home/khairi/.cache/huggingface/hub/models--HuggingFaceTB--SmolLM2-135M/snapshots/93efa2f097d58c2a74874c7e644dbc9b0cee75a2/vocab.json\n",
      "loading file merges.txt from cache at /home/khairi/.cache/huggingface/hub/models--HuggingFaceTB--SmolLM2-135M/snapshots/93efa2f097d58c2a74874c7e644dbc9b0cee75a2/merges.txt\n",
      "loading file tokenizer.json from cache at /home/khairi/.cache/huggingface/hub/models--HuggingFaceTB--SmolLM2-135M/snapshots/93efa2f097d58c2a74874c7e644dbc9b0cee75a2/tokenizer.json\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at /home/khairi/.cache/huggingface/hub/models--HuggingFaceTB--SmolLM2-135M/snapshots/93efa2f097d58c2a74874c7e644dbc9b0cee75a2/special_tokens_map.json\n",
      "loading file tokenizer_config.json from cache at /home/khairi/.cache/huggingface/hub/models--HuggingFaceTB--SmolLM2-135M/snapshots/93efa2f097d58c2a74874c7e644dbc9b0cee75a2/tokenizer_config.json\n",
      "loading file chat_template.jinja from cache at None\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "tokenizer.pad_token = tokenizer.eos_token # Set pad token to eos token for open-ended generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "92a948e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /home/khairi/.cache/huggingface/hub/models--HuggingFaceTB--SmolLM2-135M/snapshots/93efa2f097d58c2a74874c7e644dbc9b0cee75a2/config.json\n",
      "Model config LlamaConfig {\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"head_dim\": 64,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 576,\n",
      "  \"initializer_range\": 0.041666666666666664,\n",
      "  \"intermediate_size\": 1536,\n",
      "  \"is_llama_config\": true,\n",
      "  \"max_position_embeddings\": 8192,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 9,\n",
      "  \"num_hidden_layers\": 30,\n",
      "  \"num_key_value_heads\": 3,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_interleaved\": false,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 100000,\n",
      "  \"tie_word_embeddings\": true,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.52.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 49152\n",
      "}\n",
      "\n",
      "loading weights file model.safetensors from cache at /home/khairi/.cache/huggingface/hub/models--HuggingFaceTB--SmolLM2-135M/snapshots/93efa2f097d58c2a74874c7e644dbc9b0cee75a2/model.safetensors\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 0\n",
      "}\n",
      "\n",
      "All model checkpoint weights were used when initializing LlamaForCausalLM.\n",
      "\n",
      "All the weights of LlamaForCausalLM were initialized from the model checkpoint at HuggingFaceTB/SmolLM2-135M.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.\n",
      "loading configuration file generation_config.json from cache at /home/khairi/.cache/huggingface/hub/models--HuggingFaceTB--SmolLM2-135M/snapshots/93efa2f097d58c2a74874c7e644dbc9b0cee75a2/generation_config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 0\n",
      "}\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(49152, 576)\n",
       "    (layers): ModuleList(\n",
       "      (0-29): 30 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=576, out_features=576, bias=False)\n",
       "          (k_proj): Linear(in_features=576, out_features=192, bias=False)\n",
       "          (v_proj): Linear(in_features=576, out_features=192, bias=False)\n",
       "          (o_proj): Linear(in_features=576, out_features=576, bias=False)\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=576, out_features=1536, bias=False)\n",
       "          (up_proj): Linear(in_features=576, out_features=1536, bias=False)\n",
       "          (down_proj): Linear(in_features=1536, out_features=576, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((576,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((576,), eps=1e-05)\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm((576,), eps=1e-05)\n",
       "    (rotary_emb): LlamaRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=576, out_features=49152, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(model_id)\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10bd6faa",
   "metadata": {},
   "source": [
    "#### Efficient finetuning with LoRA\n",
    "##### Full Fine-tuning vs LoRA:\n",
    "Full fine-tuning updates all the parameters of a pre-trained model, which is computationally expensive and memory-intensive, especially for large models. In contrast, LoRA (Low-Rank Adaptation) introduces small adapter layers into the model and only trains these new weights, leaving the original model parameters frozen. This drastically reduces compute and storage requirements while still allowing the model to adapt to the new domain."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc552d5c",
   "metadata": {},
   "source": [
    "#### Our Approach:\n",
    "Instead of fine-tuning the entire SmolLM2 model, we train only the LoRA adapters. However, during continued pretraining on protein sequences, we also fine-tune the embedding layers to allow the model to learn representations for new tokens (i.e., amino acids) that may not have been fully captured during the original pretraining."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "05fd27c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_rank = 8\n",
    "lora_alpha = 32\n",
    "lora_dropout = 0.1\n",
    "modules = [\n",
    "    \"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\", # attention layer\n",
    "    \"up_proj\", \"down_proj\", \"gate_proj\", # feedforward layer\n",
    "    \"lm_head\", \"embed_tokens\" # update embeddings when learning a new language\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3ba5dd33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LoraConfig(task_type=<TaskType.CAUSAL_LM: 'CAUSAL_LM'>, peft_type=<PeftType.LORA: 'LORA'>, auto_mapping=None, base_model_name_or_path=None, revision=None, inference_mode=False, r=8, target_modules={'gate_proj', 'lm_head', 'down_proj', 'up_proj', 'embed_tokens', 'k_proj', 'v_proj', 'o_proj', 'q_proj'}, exclude_modules=None, lora_alpha=32, lora_dropout=0.1, fan_in_fan_out=False, bias='none', use_rslora=False, modules_to_save=None, init_lora_weights=True, layers_to_transform=None, layers_pattern=None, rank_pattern={}, alpha_pattern={}, megatron_config=None, megatron_core='megatron.core', trainable_token_indices=None, loftq_config={}, eva_config=None, corda_config=None, use_dora=False, layer_replication=None, runtime_config=LoraRuntimeConfig(ephemeral_gpu_offload=False), lora_bias=False)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lora_config = LoraConfig(\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    inference_mode=False,\n",
    "    r=lora_rank,\n",
    "    lora_alpha=lora_alpha,\n",
    "    lora_dropout=lora_dropout,\n",
    "    target_modules=modules\n",
    ")\n",
    "\n",
    "lora_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "39838299",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 3,237,888 || all params: 137,752,896 || trainable%: 2.3505\n"
     ]
    }
   ],
   "source": [
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a00f2293",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['Entry', 'Sequence'],\n",
       "        num_rows: 455692\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['Entry', 'Sequence'],\n",
       "        num_rows: 2000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['Entry', 'Sequence'],\n",
       "        num_rows: 10000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = load_dataset(dataset_id)\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "281e6131",
   "metadata": {},
   "source": [
    "### Dataset Processing Steps\n",
    "\n",
    "##### Filter by Length\n",
    "Keep only sequences with length ≤ 512 to ensure consistent input size for the model.\n",
    "\n",
    "##### Format Protein Sequences\n",
    "- Separate each amino acid with a space so that every amino acid is treated as an individual token.\n",
    "- Enclose the full sequence with special tokens <protein> at the start and </protein> at the end.\n",
    "\n",
    "Example:\n",
    "```\n",
    "Original:  MIGMLESLQH\n",
    "Formatted: <protein> M I G M L E S L Q H </protein>\n",
    "```\n",
    "##### Tokenization\n",
    "Convert the formatted sequences into input IDs using the model’s tokenizer, producing the final input ready for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9fba4c1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_sequence_by_length(example):\n",
    "    \"\"\"\n",
    "    Filter sequences longer than 512 tokens.\n",
    "\n",
    "    Args:\n",
    "        example (Dict[str, Any]): row from the dataset\n",
    "\n",
    "    Returns:\n",
    "        boolean: True if the sequence length is less than or equal to 512, False otherwise\n",
    "    \"\"\"\n",
    "    return len(example['Sequence']) <= 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e42131d",
   "metadata": {},
   "outputs": [],
   "source": [
    "eos_token = tokenizer.eos_token\n",
    "def format_sequence(example):\n",
    "    \"\"\"\n",
    "    Format the sequence by adding spaces between each character.\n",
    "    Each amino acid is treated as a separate token.\n",
    "    Add special tokens <protein> and </protein> to the start and end of the sequence.\n",
    "    Add eos token to the end of the sequence.\n",
    "\n",
    "    Args:\n",
    "        example (Dict[str, Any]): row from the dataset\n",
    "\n",
    "    Returns:\n",
    "        Dict[str, Any]: row with formatted sequence\n",
    "    \"\"\"\n",
    "    sequence = ' '.join(list(example['Sequence']))\n",
    "    sequence = f'<protein> {sequence} </protein> {eos_token}' \n",
    "    \n",
    "    return {'text': sequence}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4daefaf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_input(example):\n",
    "    \"\"\"\n",
    "    Tokenize the input sequence.\n",
    "\n",
    "    Args:\n",
    "        example (Dict[str, Any]): row from the dataset\n",
    "\n",
    "    Returns:\n",
    "        Dict[str, Any]: row with tokenized input_ids and attention_mask\n",
    "    \"\"\"\n",
    "    tokenized = tokenizer(\n",
    "        example['text'],\n",
    "        padding=True,\n",
    "    )\n",
    "    \n",
    "    return {\n",
    "        'input_ids': tokenized['input_ids'],\n",
    "        'attention_mask': tokenized['attention_mask']\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "606ec5f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89920409c1984f65ac18fb5251c602d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/455692 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'attention_mask'],\n",
       "    num_rows: 455692\n",
       "})"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = dataset['train'] \\\n",
    "    .filter(filter_sequence_by_length) \\\n",
    "    .map(format_sequence) \\\n",
    "    .map(tokenize_input, batched=True, batch_size=1024) \\\n",
    "    .select_columns(['input_ids', 'attention_mask'])\n",
    "\n",
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "bf9988df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "486adfa35c5148f59066744b63d84bcf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'attention_mask'],\n",
       "    num_rows: 2000\n",
       "})"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_data = dataset['validation'] \\\n",
    "    .filter(filter_sequence_by_length) \\\n",
    "    .map(format_sequence) \\\n",
    "    .map(tokenize_input, batched=True, batch_size=1024) \\\n",
    "    .select_columns(['input_ids', 'attention_mask'])\n",
    "    \n",
    "valid_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3ad135b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataCollatorForLanguageModeling(tokenizer=GPT2TokenizerFast(name_or_path='HuggingFaceTB/SmolLM2-135M', vocab_size=49152, model_max_length=8192, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>', 'additional_special_tokens': ['<|endoftext|>', '<|im_start|>', '<|im_end|>', '<repo_name>', '<reponame>', '<file_sep>', '<filename>', '<gh_stars>', '<issue_start>', '<issue_comment>', '<issue_closed>', '<jupyter_start>', '<jupyter_text>', '<jupyter_code>', '<jupyter_output>', '<jupyter_script>', '<empty_output>']}, clean_up_tokenization_spaces=False, added_tokens_decoder={\n",
       "\t0: AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t1: AddedToken(\"<|im_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t2: AddedToken(\"<|im_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t3: AddedToken(\"<repo_name>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t4: AddedToken(\"<reponame>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t5: AddedToken(\"<file_sep>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t6: AddedToken(\"<filename>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t7: AddedToken(\"<gh_stars>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t8: AddedToken(\"<issue_start>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t9: AddedToken(\"<issue_comment>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t10: AddedToken(\"<issue_closed>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t11: AddedToken(\"<jupyter_start>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t12: AddedToken(\"<jupyter_text>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t13: AddedToken(\"<jupyter_code>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t14: AddedToken(\"<jupyter_output>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t15: AddedToken(\"<jupyter_script>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t16: AddedToken(\"<empty_output>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}\n",
       "), mlm=False, mlm_probability=0.15, mask_replace_prob=0.8, random_replace_prob=0.1, pad_to_multiple_of=None, tf_experimental_compile=False, return_tensors='pt', seed=None)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "\n",
    "data_collator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40c5ab35",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = \"/tmp/pilm-builder-results\"\n",
    "batch_size = 4\n",
    "max_epochs = 3\n",
    "learning_rate = 2e-4\n",
    "weight_decay = 0.01\n",
    "warmup_ratio = 0.15\n",
    "use_fp16 = torch.cuda.is_available()\n",
    "run_name = \"cpt-smollm-swissprot-lora\"\n",
    "gradient_accumulation_steps = 2  # to simulate a larger batch size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "012b3158",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    do_train=True,\n",
    "    do_eval=True,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "    # Logging & evaluation config\n",
    "    output_dir=output_dir,\n",
    "    eval_strategy=\"steps\",\n",
    "    save_strategy=\"steps\",\n",
    "    eval_steps=100,\n",
    "    save_steps=100,\n",
    "    logging_steps=50,\n",
    "    save_total_limit=3,\n",
    "    num_train_epochs=max_epochs,\n",
    "    # Optimizer config\n",
    "    optim=OptimizerNames.ADAMW_TORCH,\n",
    "    learning_rate=learning_rate,\n",
    "    weight_decay=weight_decay,\n",
    "    warmup_ratio=warmup_ratio,\n",
    "    # fp16\n",
    "    fp16=use_fp16,\n",
    "    run_name=run_name,\n",
    "    report_to=\"none\",  # change to \"wandb\" to enable logging to Weights & Biases\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    greater_is_better=False,\n",
    "    # Uncomment the following lines to enable pushing to the HuggingFace Hub\n",
    "    # push_to_hub=True,\n",
    "    # hub_model_id=\"<your-username>/cpt-smollm-swissprot-lora\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7949b12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment the following line to use full data\n",
    "train_data = train_data.select(range(1000))  # use a smaller subset for testing\n",
    "valid_data = valid_data.select(range(100))  # use a smaller subset for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "57a2136a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using auto half precision backend\n",
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_data,\n",
    "    eval_dataset=valid_data,\n",
    "    data_collator=data_collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "62ea9358",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num examples = 1,000\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 4\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 4\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 750\n",
      "  Number of trainable parameters = 3,237,888\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='750' max='750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [750/750 17:17, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>2.869400</td>\n",
       "      <td>2.836969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>2.827000</td>\n",
       "      <td>2.825628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>2.823500</td>\n",
       "      <td>2.816247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>2.806900</td>\n",
       "      <td>2.813277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>2.813000</td>\n",
       "      <td>2.811435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>2.807000</td>\n",
       "      <td>2.810436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>2.808600</td>\n",
       "      <td>2.808460</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2000\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to /tmp/pilm-builder-results/checkpoint-100\n",
      "/home/khairi/.conda/envs/torch/lib/python3.10/site-packages/peft/utils/save_and_load.py:220: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
      "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n",
      "Saving Trainer.data_collator.tokenizer by default as Trainer.processing_class is `None`\n",
      "tokenizer config file saved in /tmp/pilm-builder-results/checkpoint-100/tokenizer_config.json\n",
      "Special tokens file saved in /tmp/pilm-builder-results/checkpoint-100/special_tokens_map.json\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2000\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to /tmp/pilm-builder-results/checkpoint-200\n",
      "/home/khairi/.conda/envs/torch/lib/python3.10/site-packages/peft/utils/save_and_load.py:220: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
      "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n",
      "Saving Trainer.data_collator.tokenizer by default as Trainer.processing_class is `None`\n",
      "tokenizer config file saved in /tmp/pilm-builder-results/checkpoint-200/tokenizer_config.json\n",
      "Special tokens file saved in /tmp/pilm-builder-results/checkpoint-200/special_tokens_map.json\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2000\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to /tmp/pilm-builder-results/checkpoint-300\n",
      "/home/khairi/.conda/envs/torch/lib/python3.10/site-packages/peft/utils/save_and_load.py:220: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
      "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n",
      "Saving Trainer.data_collator.tokenizer by default as Trainer.processing_class is `None`\n",
      "tokenizer config file saved in /tmp/pilm-builder-results/checkpoint-300/tokenizer_config.json\n",
      "Special tokens file saved in /tmp/pilm-builder-results/checkpoint-300/special_tokens_map.json\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2000\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to /tmp/pilm-builder-results/checkpoint-400\n",
      "/home/khairi/.conda/envs/torch/lib/python3.10/site-packages/peft/utils/save_and_load.py:220: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
      "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n",
      "Saving Trainer.data_collator.tokenizer by default as Trainer.processing_class is `None`\n",
      "tokenizer config file saved in /tmp/pilm-builder-results/checkpoint-400/tokenizer_config.json\n",
      "Special tokens file saved in /tmp/pilm-builder-results/checkpoint-400/special_tokens_map.json\n",
      "Deleting older checkpoint [/tmp/pilm-builder-results/checkpoint-100] due to args.save_total_limit\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2000\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to /tmp/pilm-builder-results/checkpoint-500\n",
      "/home/khairi/.conda/envs/torch/lib/python3.10/site-packages/peft/utils/save_and_load.py:220: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
      "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n",
      "Saving Trainer.data_collator.tokenizer by default as Trainer.processing_class is `None`\n",
      "tokenizer config file saved in /tmp/pilm-builder-results/checkpoint-500/tokenizer_config.json\n",
      "Special tokens file saved in /tmp/pilm-builder-results/checkpoint-500/special_tokens_map.json\n",
      "Deleting older checkpoint [/tmp/pilm-builder-results/checkpoint-200] due to args.save_total_limit\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2000\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to /tmp/pilm-builder-results/checkpoint-600\n",
      "/home/khairi/.conda/envs/torch/lib/python3.10/site-packages/peft/utils/save_and_load.py:220: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
      "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n",
      "Saving Trainer.data_collator.tokenizer by default as Trainer.processing_class is `None`\n",
      "tokenizer config file saved in /tmp/pilm-builder-results/checkpoint-600/tokenizer_config.json\n",
      "Special tokens file saved in /tmp/pilm-builder-results/checkpoint-600/special_tokens_map.json\n",
      "Deleting older checkpoint [/tmp/pilm-builder-results/checkpoint-300] due to args.save_total_limit\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2000\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to /tmp/pilm-builder-results/checkpoint-700\n",
      "/home/khairi/.conda/envs/torch/lib/python3.10/site-packages/peft/utils/save_and_load.py:220: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
      "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n",
      "Saving Trainer.data_collator.tokenizer by default as Trainer.processing_class is `None`\n",
      "tokenizer config file saved in /tmp/pilm-builder-results/checkpoint-700/tokenizer_config.json\n",
      "Special tokens file saved in /tmp/pilm-builder-results/checkpoint-700/special_tokens_map.json\n",
      "Deleting older checkpoint [/tmp/pilm-builder-results/checkpoint-400] due to args.save_total_limit\n",
      "Saving model checkpoint to /tmp/pilm-builder-results/checkpoint-750\n",
      "/home/khairi/.conda/envs/torch/lib/python3.10/site-packages/peft/utils/save_and_load.py:220: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
      "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n",
      "Saving Trainer.data_collator.tokenizer by default as Trainer.processing_class is `None`\n",
      "tokenizer config file saved in /tmp/pilm-builder-results/checkpoint-750/tokenizer_config.json\n",
      "Special tokens file saved in /tmp/pilm-builder-results/checkpoint-750/special_tokens_map.json\n",
      "Deleting older checkpoint [/tmp/pilm-builder-results/checkpoint-500] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from /tmp/pilm-builder-results/checkpoint-700 (score: 2.8084604740142822).\n"
     ]
    }
   ],
   "source": [
    "training_stats = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "7c70857b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_runtime: 1037.9977\n",
      "train_samples_per_second: 2.89\n",
      "train_steps_per_second: 0.723\n",
      "total_flos: 1022401035648000.0\n",
      "train_loss: 2.841126180013021\n",
      "epoch: 3.0\n"
     ]
    }
   ],
   "source": [
    "for key, value in training_stats.metrics.items():\n",
    "    print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65fcd634",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment the following line to save the model locally and/or push to the HuggingFace Hub\n",
    "# model.save_pretrained(\"/path/to/save/model\")\n",
    "# model.push_to_hub(\"<username>/<model-id>\", private=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfbe004d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
