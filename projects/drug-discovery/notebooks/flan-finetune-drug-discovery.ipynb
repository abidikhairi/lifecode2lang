{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Fine tuning FLAN-T5 on Drug Discovery\n\nAuthor: [Khairi Abidi](https://github.com/abidikhairi)\n\nThis notebook demonstrates finetuning of FLAN-T5 for drug discovery instruction.\n\nKey Features:\n\n- Memory Efficient: LoRA for consumer GPUs\n\nThe model learns to recommend adequate drugs based on gene context.","metadata":{}},{"cell_type":"code","source":"%env CUDA_VISIBLE_DEVICES=0,1","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%env WANDB_PROJECT=Drug-FLAN","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Connect to 3rd party services\n\n- **WandB**: for experiment tracking.\n- **HuggingFace Hub**: for model checkpoints uploading.","metadata":{}},{"cell_type":"code","source":"from kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\nhf_token = user_secrets.get_secret(\"HUGGING_FACE_TOKEN\")\nwandb_token = user_secrets.get_secret(\"WANDB_API_KEY\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!wandb login {wandb_token}","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!huggingface-cli login --token {hf_token}","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## GPU Environment Detection\nVerify GPU availability and display hardware specifications for optimal training configuration.","metadata":{}},{"cell_type":"code","source":"import torch\n\n# Verify CUDA availability and display GPU specifications\nprint(f\"CUDA available: {torch.cuda.is_available()}\")\nprint(f\"Number of GPUs: {torch.cuda.device_count()}\")\n\nif torch.cuda.is_available():\n    # Display current GPU details for training optimization\n    print(f\"Current GPU: {torch.cuda.current_device()}\")\n    print(f\"GPU name: {torch.cuda.get_device_name()}\")\n    print(f\"GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\nelse:\n    # Provide guidance for enabling GPU in Colab\n    print(\"‚ö†Ô∏è  No GPU available. This notebook requires a GPU for efficient training.\")\n    print(\"In Colab: Runtime ‚Üí Change runtime type ‚Üí Hardware accelerator ‚Üí GPU\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Core Library Imports\nImport essential libraries for finetuning, model configuration, and experiment tracking.","metadata":{}},{"cell_type":"code","source":"# Model and tokenization\nfrom transformers import (\n    T5ForConditionalGeneration,    # Seq2Seq language model loading\n    T5Tokenizer,                   # Text tokenization\n    DataCollatorForSeq2Seq,        # Batch inputs handling\n)\n\n# Model optimization\nfrom torch.optim import AdamW\nfrom transformers import get_scheduler\n\n# Training and Setup\nfrom transformers import (\n    Seq2SeqTrainer,\n    Seq2SeqTrainingArguments,\n)\n\n# Dataset handling\nfrom datasets import load_dataset\n\n# Logging configuration\nimport logging\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model_name = \"google/flan-t5-base\"\nmax_seq_length = 512\n\nprint(f\"Loading model: {model_name}\")\nprint(f\"Max sequence length: {max_seq_length}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model = T5ForConditionalGeneration.from_pretrained(\n    model_name,\n    trust_remote_code=True,               # Allow custom model code execution\n    # dtype=torch.float16,                  # Use FP16 for non-quantized operations\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"tokenizer = T5Tokenizer.from_pretrained(model_name, trust_remote_code=True)\n\n# Ensure tokenizer has proper padding token for batch processing\nif tokenizer.pad_token is None:\n    tokenizer.pad_token = tokenizer.eos_token","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(f\"‚úÖ Model loaded successfully!\")\nprint(f\"üìä Model parameters: ~{sum(p.numel() for p in model.parameters()) / 1e6:.1f}M\")\nprint(f\"üßÆ Quantized parameters: ~{sum(p.numel() for p in model.parameters() if hasattr(p, 'quant_type')) / 1e6:.1f}M\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def compute_model_size(model):\n    n_params = 0\n    for p in model.parameters():\n        n_params += p.nelement() * p.element_size()\n    for p in model.buffers():\n        n_params += p.nelement() * p.element_size()\n\n    return n_params / (1024 ** 3)\n\nprint(f\"üìä Model size : {compute_model_size(model):.2f} GB\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Dataset Setup\nConfigure the Drug discovery instruction dataset.","metadata":{}},{"cell_type":"code","source":"def tokenize_dataset_example(examples):\n    model_inputs = tokenizer(examples['input'], return_tensors='pt', padding=True)\n    labels = tokenizer(examples['target'], return_tensors='pt', padding=True)\n\n    model_inputs['labels'] = labels['input_ids']\n    return model_inputs\n\nprint(\"‚úÖ Dataset tokenization function defined (batch mode)\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def filter_dataset_example(example):\n    return len(example['input_ids']) < 512\n    \nprint(\"‚úÖ Dataset filter functions defined\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Load and preprocess DrugInstruct training dataset\nprint(\"üîÑ Loading DrugInstruct dataset...\")\ndataset = load_dataset(\"khairi/drug-discovery-hetionet\")\n\n# Apply conversation formatting to all examples\ndataset = dataset.map(tokenize_dataset_example, batched=True, batch_size=4)\n    # .filter(filter_dataset_example)\n\ntrain_data = dataset['train']\nvalid_data = dataset['validation']\n\nprint(f\"‚úÖ Dataset loaded and processed!\")\nprint(f\"üìä Training examples: {len(train_data):,}\")\nprint(f\"üìä Validation examples: {len(valid_data):,}\")\nprint(f\"üéØ Sample protein: {tokenizer.decode(train_data[0]['input_ids'])}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Training Setup\nConfigure training parameters optimized for finetuning with memory constraints.","metadata":{}},{"cell_type":"code","source":"train_data","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"valid_data","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Prepare data collator\ndata_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Configure Finetuning training parameters for sequence modeling\ntraining_args = Seq2SeqTrainingArguments(\n    # Memory-efficient batch configuration\n    per_device_train_batch_size=8,   # Small batch for GPU memory constraints\n    gradient_accumulation_steps=4,   # Effective batch size = 8 * 4 = 16\n       \n    # Training duration and monitoring\n    # max_steps=50,                     # Short demo run (increase to 500+ for production)\n    logging_steps=100,                  # Log metrics every step for close monitoring\n    save_steps=100,\n    eval_steps=100,\n    eval_strategy='steps',\n    num_train_epochs=5,            # Uncomment for production\n    \n    save_total_limit = 3,\n    \n    # Stability and output configuration\n    output_dir=\"./finetuning_outputs\",\n    max_grad_norm=0.1,               # Aggressive gradient clipping for stable training\n    report_to=\"none\",                # use wandb for experiment tracking\n    run_name='flan-base-hetionet-instruct',\n\n    # Push to Hub, uncomment in production\n    push_to_hub=True,\n    hub_model_id='khairi/Shizuku-0.5B'\n    \n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Optimizer and Scheduler Setup\nConfigure **AdamW** optimizer with for model parameters.\n\nThen sets up a cosine learning rate scheduler with a short warmup (fast warmup/slow cooldown) and total training steps.","metadata":{}},{"cell_type":"code","source":"print(\"üöÄ Initializing optimizer...\")\nprint(\"üåô Setting up cosine LR scheduler...\")\n\noptimizer = AdamW([\n        {'params': model.parameters(), 'lr': 1e-4},\n    ],\n    betas=(0.99, 0.98),\n    weight_decay=0.01\n)\n\nlr_scheduler = get_scheduler(\n    name='cosine',\n    optimizer=optimizer,\n    num_warmup_steps=10,\n    num_training_steps=50\n)\n\nprint(\"‚úÖ Optimizer ready!\")\nprint(\"‚ú® LR scheduler ready!\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"trainer = Seq2SeqTrainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_data,\n    eval_dataset=valid_data,\n    tokenizer=tokenizer,\n    data_collator=data_collator,\n    optimizers=(optimizer, lr_scheduler)\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Execute FineTuning\nprint(\"üöÄ Starting Finetuning...\")\n\n# Run the training process\ntrainer.train()\n\nprint(\"‚úÖ Training completed successfully!\")\nprint(f\"üíæ Model saved to: {training_args.output_dir}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"row = valid_data[0]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"row.keys()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"inputs = tokenizer(row['input'], return_tensors='pt')\n\ninputs = {k: v.to(model.device) for k, v in inputs.items()}","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"output = model.generate(**inputs)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(tokenizer.decode(output[0]))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(row['target'])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}