{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Fine tuning FLAN-T5 on Drug Discovery\n\nAuthor: [Khairi Abidi](https://github.com/abidikhairi)\n\nThis notebook demonstrates finetuning of FLAN-T5 for drug discovery instruction.\n\nKey Features:\n\n- Memory Efficient: LoRA for consumer GPUs\n\nThe model learns to recommend adequate drugs based on gene context.","metadata":{}},{"cell_type":"markdown","source":"## Installation and Setup\n\nInstall the required packages for finetuning with memory-efficient techniques.","metadata":{}},{"cell_type":"code","source":"%%capture\n!pip install --quiet transformers datasets trl bitsandbytes peft trackio","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-25T08:15:29.964441Z","iopub.execute_input":"2025-09-25T08:15:29.964777Z","iopub.status.idle":"2025-09-25T08:17:49.326137Z","shell.execute_reply.started":"2025-09-25T08:15:29.964750Z","shell.execute_reply":"2025-09-25T08:17:49.324332Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"%env CUDA_VISIBLE_DEVICES=0,1","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-25T08:17:49.329324Z","iopub.execute_input":"2025-09-25T08:17:49.329826Z","iopub.status.idle":"2025-09-25T08:17:49.339948Z","shell.execute_reply.started":"2025-09-25T08:17:49.329753Z","shell.execute_reply":"2025-09-25T08:17:49.338240Z"}},"outputs":[{"name":"stdout","text":"env: CUDA_VISIBLE_DEVICES=0,1\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"%env WANDB_PROJECT=Drug-FLAN","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-25T08:18:08.327304Z","iopub.execute_input":"2025-09-25T08:18:08.327637Z","iopub.status.idle":"2025-09-25T08:18:08.334117Z","shell.execute_reply.started":"2025-09-25T08:18:08.327602Z","shell.execute_reply":"2025-09-25T08:18:08.332917Z"}},"outputs":[{"name":"stdout","text":"env: WANDB_PROJECT=Drug-FLAN\n","output_type":"stream"}],"execution_count":4},{"cell_type":"markdown","source":"## Connect to 3rd party services\n\n- **WandB**: for experiment tracking.\n- **HuggingFace Hub**: for model checkpoints uploading.","metadata":{}},{"cell_type":"code","source":"from kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\nhf_token = user_secrets.get_secret(\"HUGGING_FACE_TOKEN\")\nwandb_token = user_secrets.get_secret(\"WANDB_API_KEY\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-25T08:18:16.924491Z","iopub.execute_input":"2025-09-25T08:18:16.924880Z","iopub.status.idle":"2025-09-25T08:18:17.148011Z","shell.execute_reply.started":"2025-09-25T08:18:16.924854Z","shell.execute_reply":"2025-09-25T08:18:17.146867Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"!wandb login {wandb_token}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-25T08:18:28.217534Z","iopub.execute_input":"2025-09-25T08:18:28.217840Z","iopub.status.idle":"2025-09-25T08:18:33.438367Z","shell.execute_reply.started":"2025-09-25T08:18:28.217818Z","shell.execute_reply":"2025-09-25T08:18:33.437232Z"}},"outputs":[{"name":"stdout","text":"\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n\u001b[34m\u001b[1mwandb\u001b[0m: W&B API key is configured. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"!huggingface-cli login --token {hf_token}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-25T08:18:39.149166Z","iopub.execute_input":"2025-09-25T08:18:39.150233Z","iopub.status.idle":"2025-09-25T08:18:40.000819Z","shell.execute_reply.started":"2025-09-25T08:18:39.150164Z","shell.execute_reply":"2025-09-25T08:18:39.999761Z"}},"outputs":[{"name":"stdout","text":"\u001b[33m‚ö†Ô∏è  Warning: 'huggingface-cli login' is deprecated. Use 'hf auth login' instead.\u001b[0m\nThe token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `hf`CLI if you want to set the git credential as well.\nToken is valid (permission: write).\nThe token `KAGGLE_TOKEN` has been saved to /root/.cache/huggingface/stored_tokens\nYour token has been saved to /root/.cache/huggingface/token\nLogin successful.\nThe current active token is: `KAGGLE_TOKEN`\n","output_type":"stream"}],"execution_count":8},{"cell_type":"markdown","source":"## GPU Environment Detection\nVerify GPU availability and display hardware specifications for optimal training configuration.","metadata":{}},{"cell_type":"code","source":"import torch\n\n# Verify CUDA availability and display GPU specifications\nprint(f\"CUDA available: {torch.cuda.is_available()}\")\nprint(f\"Number of GPUs: {torch.cuda.device_count()}\")\n\nif torch.cuda.is_available():\n    # Display current GPU details for training optimization\n    print(f\"Current GPU: {torch.cuda.current_device()}\")\n    print(f\"GPU name: {torch.cuda.get_device_name()}\")\n    print(f\"GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\nelse:\n    # Provide guidance for enabling GPU in Colab\n    print(\"‚ö†Ô∏è  No GPU available. This notebook requires a GPU for efficient training.\")\n    print(\"In Colab: Runtime ‚Üí Change runtime type ‚Üí Hardware accelerator ‚Üí GPU\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-25T08:19:17.745145Z","iopub.execute_input":"2025-09-25T08:19:17.745546Z","iopub.status.idle":"2025-09-25T08:19:22.282779Z","shell.execute_reply.started":"2025-09-25T08:19:17.745511Z","shell.execute_reply":"2025-09-25T08:19:22.281303Z"}},"outputs":[{"name":"stdout","text":"CUDA available: False\nNumber of GPUs: 0\n‚ö†Ô∏è  No GPU available. This notebook requires a GPU for efficient training.\nIn Colab: Runtime ‚Üí Change runtime type ‚Üí Hardware accelerator ‚Üí GPU\n","output_type":"stream"}],"execution_count":9},{"cell_type":"markdown","source":"## Core Library Imports\nImport essential libraries for finetuning, model configuration, and experiment tracking.","metadata":{}},{"cell_type":"code","source":"# Model and tokenization\nfrom transformers import (\n    T5ForConditionalGeneration,    # Seq2Seq language model loading\n    T5Tokenizer,                   # Text tokenization\n    DataCollatorForSeq2Seq,        # Batch inputs handling\n)\n\n# Model optimization\nfrom torch.optim import AdamW\nfrom transformers import get_scheduler\n\n# Training and Setup\nfrom transformers import (\n    Seq2SeqTrainer,\n    Seq2SeqTrainingArguments,\n)\n\n# Dataset handling\nfrom datasets import load_dataset\n\n# Logging configuration\nimport logging\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-09-25T08:22:27.037839Z","iopub.execute_input":"2025-09-25T08:22:27.038455Z","iopub.status.idle":"2025-09-25T08:23:00.363398Z","shell.execute_reply.started":"2025-09-25T08:22:27.038424Z","shell.execute_reply":"2025-09-25T08:23:00.362357Z"}},"outputs":[{"name":"stderr","text":"2025-09-25 08:22:38.349056: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1758788558.605203      36 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1758788558.690544      36 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"model_name = \"google/flan-t5-base\"\nmax_seq_length = 1024\n\nprint(f\"Loading model: {model_name}\")\nprint(f\"Max sequence length: {max_seq_length}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-25T08:23:39.764830Z","iopub.execute_input":"2025-09-25T08:23:39.765283Z","iopub.status.idle":"2025-09-25T08:23:39.771268Z","shell.execute_reply.started":"2025-09-25T08:23:39.765249Z","shell.execute_reply":"2025-09-25T08:23:39.770199Z"}},"outputs":[{"name":"stdout","text":"Loading model: google/flan-t5-base\nMax sequence length: 1024\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"model = T5ForConditionalGeneration.from_pretrained(\n    model_name,\n    trust_remote_code=True,               # Allow custom model code execution\n    dtype=torch.float16,                 # Use FP16 for non-quantized operations\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-25T08:24:34.092563Z","iopub.execute_input":"2025-09-25T08:24:34.093002Z","iopub.status.idle":"2025-09-25T08:24:35.235528Z","shell.execute_reply.started":"2025-09-25T08:24:34.092976Z","shell.execute_reply":"2025-09-25T08:24:35.234367Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"tokenizer = T5Tokenizer.from_pretrained(model_name, trust_remote_code=True)\n\n# Ensure tokenizer has proper padding token for batch processing\nif tokenizer.pad_token is None:\n    tokenizer.pad_token = tokenizer.eos_token","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-25T08:25:16.358611Z","iopub.execute_input":"2025-09-25T08:25:16.359124Z","iopub.status.idle":"2025-09-25T08:25:17.368283Z","shell.execute_reply.started":"2025-09-25T08:25:16.359091Z","shell.execute_reply":"2025-09-25T08:25:17.366998Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"print(f\"‚úÖ Model loaded successfully!\")\nprint(f\"üìä Model parameters: ~{sum(p.numel() for p in model.parameters()) / 1e6:.1f}M\")\nprint(f\"üßÆ Quantized parameters: ~{sum(p.numel() for p in model.parameters() if hasattr(p, 'quant_type')) / 1e6:.1f}M\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-25T08:25:58.426765Z","iopub.execute_input":"2025-09-25T08:25:58.427142Z","iopub.status.idle":"2025-09-25T08:25:58.436465Z","shell.execute_reply.started":"2025-09-25T08:25:58.427115Z","shell.execute_reply":"2025-09-25T08:25:58.435321Z"}},"outputs":[{"name":"stdout","text":"‚úÖ Model loaded successfully!\nüìä Model parameters: ~247.6M\nüßÆ Quantized parameters: ~0.0M\n","output_type":"stream"}],"execution_count":20},{"cell_type":"code","source":"def compute_model_size(model):\n    n_params = 0\n    for p in model.parameters():\n        n_params += p.nelement() * p.element_size()\n    for p in model.buffers():\n        n_params += p.nelement() * p.element_size()\n\n    return n_params / (1024 ** 3)\n\nprint(f\"üìä Model size : {compute_model_size(model):.2f} GB\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-25T08:26:11.773795Z","iopub.execute_input":"2025-09-25T08:26:11.774178Z","iopub.status.idle":"2025-09-25T08:26:11.784463Z","shell.execute_reply.started":"2025-09-25T08:26:11.774153Z","shell.execute_reply":"2025-09-25T08:26:11.783294Z"}},"outputs":[{"name":"stdout","text":"üìä Model size : 0.53 GB\n","output_type":"stream"}],"execution_count":21},{"cell_type":"markdown","source":"## Dataset Setup\nConfigure the Drug discovery instruction dataset.","metadata":{}},{"cell_type":"code","source":"def tokenize_dataset_example(examples):\n    model_inputs = tokenizer(examples['input'], return_tensors='pt', padding=True)\n    labels = tokenizer(examples['target'], return_tensors='pt', padding=True)\n\n    model_inputs['labels'] = labels['input_ids']\n    return model_inputs\n\nprint(\"‚úÖ Dataset tokenization function defined (batch mode)\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-25T08:54:02.870746Z","iopub.execute_input":"2025-09-25T08:54:02.871201Z","iopub.status.idle":"2025-09-25T08:54:02.878603Z","shell.execute_reply.started":"2025-09-25T08:54:02.871177Z","shell.execute_reply":"2025-09-25T08:54:02.877473Z"}},"outputs":[{"name":"stdout","text":"‚úÖ Dataset tokenization function defined (batch mode)\n","output_type":"stream"}],"execution_count":32},{"cell_type":"code","source":"def filter_dataset_example(example):\n    return example['input_ids'].shape[1] <= 512\n    \nprint(\"‚úÖ Dataset filter functions defined\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-25T08:54:03.018247Z","iopub.execute_input":"2025-09-25T08:54:03.018728Z","iopub.status.idle":"2025-09-25T08:54:03.025150Z","shell.execute_reply.started":"2025-09-25T08:54:03.018569Z","shell.execute_reply":"2025-09-25T08:54:03.024051Z"}},"outputs":[{"name":"stdout","text":"‚úÖ Dataset filter functions defined\n","output_type":"stream"}],"execution_count":33},{"cell_type":"code","source":"# Load and preprocess DrugInstruct training dataset\nprint(\"üîÑ Loading DrugInstruct dataset...\")\ndataset = load_dataset(\"khairi/drug-discovery-hetionet\")\n\n# Apply conversation formatting to all examples\ndataset = dataset.map(tokenize_dataset_example, batched=True, batch_size=32)\n\ntrain_data = dataset['train']\nvalid_data = dataset['validation']\n\nprint(f\"‚úÖ Dataset loaded and processed!\")\nprint(f\"üìä Training examples: {len(train_data):,}\")\nprint(f\"üìä Validation examples: {len(valid_data):,}\")\nprint(f\"üéØ Sample protein: {tokenizer.decode(train_data[0]['input_ids'])}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-25T08:54:22.489154Z","iopub.execute_input":"2025-09-25T08:54:22.489541Z","iopub.status.idle":"2025-09-25T08:54:23.649650Z","shell.execute_reply.started":"2025-09-25T08:54:22.489515Z","shell.execute_reply":"2025-09-25T08:54:23.648517Z"}},"outputs":[{"name":"stdout","text":"üîÑ Loading DrugInstruct dataset...\n‚úÖ Dataset loaded and processed!\nüìä Training examples: 1,589\nüìä Validation examples: 100\nüéØ Sample protein: Predict drugs for the central gene based on its neighborhood: Central node: NAT2 One-hop neighbors: <unk> process, cellular response to xenobiotic stimulus>, <unk> process, xenobiotic metabolic process>, <unk> process, response to xenobiotic stimulus> <unk> molecular function, acetyltransferase activity>, <unk> molecular function, N-acyltransferase activity>, <unk> molecular function, transferase activity, transferring acyl groups>, <unk> molecular function, N-acetyltransferase activity>, <unk> molecular function, arylamine N-acetyltransferase activity> Answer:</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n","output_type":"stream"}],"execution_count":35},{"cell_type":"markdown","source":"## Training Setup\nConfigure training parameters optimized for finetuning with memory constraints.","metadata":{}},{"cell_type":"code","source":"# Prepare data collator\ndata_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-25T08:57:09.531581Z","iopub.execute_input":"2025-09-25T08:57:09.532081Z","iopub.status.idle":"2025-09-25T08:57:09.537168Z","shell.execute_reply.started":"2025-09-25T08:57:09.532049Z","shell.execute_reply":"2025-09-25T08:57:09.536182Z"}},"outputs":[],"execution_count":37},{"cell_type":"code","source":"# Configure Finetuning training parameters for sequence modeling\ntraining_args = Seq2SeqTrainingArguments(\n    # Memory-efficient batch configuration\n    per_device_train_batch_size=8,   # Small batch for GPU memory constraints\n    gradient_accumulation_steps=8,   # Effective batch size = 8 * 4 = 32\n       \n    # Training duration and monitoring\n    max_steps=50,                     # Short demo run (increase to 500+ for production)\n    logging_steps=5,                  # Log metrics every step for close monitoring\n    save_steps=5,\n    eval_steps=5,\n    eval_strategy='steps',\n    # num_train_epochs=1,            # Uncomment for production\n    \n    # Stability and output configuration\n    output_dir=\"./finetuning_outputs\",\n    max_grad_norm=0.1,               # Aggressive gradient clipping for stable training\n    report_to=\"none\",                # use wandb for experiment tracking\n    run_name='flan-base-hetionet-instruct',\n\n    # Push to Hub, uncomment in production\n    #push_to_hub=True,\n    #hub_model_id='khairi/Shizuku-0.5B'\n    \n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-25T09:40:40.226625Z","iopub.execute_input":"2025-09-25T09:40:40.226999Z","iopub.status.idle":"2025-09-25T09:40:40.234906Z","shell.execute_reply.started":"2025-09-25T09:40:40.226977Z","shell.execute_reply":"2025-09-25T09:40:40.233705Z"}},"outputs":[],"execution_count":43},{"cell_type":"markdown","source":"### Optimizer and Scheduler Setup\nConfigure **AdamW** optimizer with for model parameters.\n\nThen sets up a cosine learning rate scheduler with a short warmup (fast warmup/slow cooldown) and total training steps.","metadata":{}},{"cell_type":"code","source":"print(\"üöÄ Initializing optimizer...\")\nprint(\"üåô Setting up cosine LR scheduler...\")\n\noptimizer = AdamW([\n        {'params': model.parameters(), 'lr': 3e-4},\n    ],\n    betas=(0.99, 0.98),\n    weight_decay=0.01\n)\n\nlr_scheduler = get_scheduler(\n    name='cosine',\n    optimizer=optimizer,\n    num_warmup_steps=50,\n    num_training_steps=1000\n)\n\nprint(\"‚úÖ Optimizer ready!\")\nprint(\"‚ú® LR scheduler ready!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-25T10:03:15.657766Z","iopub.execute_input":"2025-09-25T10:03:15.658117Z","iopub.status.idle":"2025-09-25T10:03:15.670022Z","shell.execute_reply.started":"2025-09-25T10:03:15.658093Z","shell.execute_reply":"2025-09-25T10:03:15.668790Z"}},"outputs":[{"name":"stdout","text":"üöÄ Initializing optimizer...\nüåô Setting up cosine LR scheduler...\n‚úÖ Optimizer ready!\n‚ú® LR scheduler ready!\n","output_type":"stream"}],"execution_count":46},{"cell_type":"code","source":"trainer = Seq2SeqTrainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_data,\n    eval_dataset=valid_data,\n    tokenizer=tokenizer,\n    data_collator=data_collator,\n    optimizers=(optimizer, lr_scheduler)\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-25T10:03:47.079255Z","iopub.execute_input":"2025-09-25T10:03:47.079739Z","iopub.status.idle":"2025-09-25T10:03:47.134743Z","shell.execute_reply.started":"2025-09-25T10:03:47.079707Z","shell.execute_reply":"2025-09-25T10:03:47.133520Z"}},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_36/3227248477.py:1: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n  trainer = Seq2SeqTrainer(\n","output_type":"stream"}],"execution_count":47},{"cell_type":"code","source":"# Execute FineTuning\nprint(\"üöÄ Starting Finetuning...\")\n\n# Run the training process\ntrainer.train()\n\nprint(\"‚úÖ Training completed successfully!\")\nprint(f\"üíæ Model saved to: {training_args.output_dir}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-25T10:04:30.590939Z","iopub.execute_input":"2025-09-25T10:04:30.591308Z","execution_failed":"2025-09-25T10:15:10.903Z"}},"outputs":[{"name":"stdout","text":"üöÄ Starting Finetuning...\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}