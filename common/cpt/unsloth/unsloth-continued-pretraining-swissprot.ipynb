{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"Continued Pretraining of Qwen-0.5B On Swissprot Sequences\nAuthor: [Khairi Abidi](https://github.com/abidikhairi/)\n\nThis notebook demonstrates continued pretraining for protein sequence modeling.\n\nKey Features:\n\n- Memory Efficient: LoRA for consumer GPUs.\n\nThe model learns to generate model/functional protein sequences.","metadata":{}},{"cell_type":"markdown","source":"## Installation and Setup\nInstall the required packages for continued pretraining with memory-efficient techniques.","metadata":{}},{"cell_type":"code","source":"%env WANDB_PROJECT=Unsloth-CPT","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-09-25T18:57:04.151972Z","iopub.execute_input":"2025-09-25T18:57:04.152206Z","iopub.status.idle":"2025-09-25T18:57:04.161697Z","shell.execute_reply.started":"2025-09-25T18:57:04.152189Z","shell.execute_reply":"2025-09-25T18:57:04.161098Z"}},"outputs":[{"name":"stdout","text":"env: WANDB_PROJECT=Unsloth-CPT\n","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"## Connect to 3rd party services\n\n- **WandB**: for experiment tracking.\n- **HuggingFace Hub**: for model checkpoints uploading.","metadata":{}},{"cell_type":"code","source":"from kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\nhf_token = user_secrets.get_secret(\"HUGGING_FACE_TOKEN\")\nwandb_token = user_secrets.get_secret(\"WANDB_API_KEY\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-25T18:57:06.761706Z","iopub.execute_input":"2025-09-25T18:57:06.761961Z","iopub.status.idle":"2025-09-25T18:57:06.933104Z","shell.execute_reply.started":"2025-09-25T18:57:06.761942Z","shell.execute_reply":"2025-09-25T18:57:06.932395Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"!wandb login {wandb_token}\n!huggingface-cli login --token {hf_token}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-25T18:57:08.852111Z","iopub.execute_input":"2025-09-25T18:57:08.852835Z","iopub.status.idle":"2025-09-25T18:57:11.223161Z","shell.execute_reply.started":"2025-09-25T18:57:08.852811Z","shell.execute_reply":"2025-09-25T18:57:11.222473Z"}},"outputs":[{"name":"stdout","text":"\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n\u001b[34m\u001b[1mwandb\u001b[0m: W&B API key is configured. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n\u001b[33m‚ö†Ô∏è  Warning: 'huggingface-cli login' is deprecated. Use 'hf auth login' instead.\u001b[0m\nThe token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `hf`CLI if you want to set the git credential as well.\nToken is valid (permission: write).\nThe token `KAGGLE_TOKEN` has been saved to /root/.cache/huggingface/stored_tokens\nYour token has been saved to /root/.cache/huggingface/token\nLogin successful.\nThe current active token is: `KAGGLE_TOKEN`\n","output_type":"stream"}],"execution_count":3},{"cell_type":"markdown","source":"## GPU Environment Detection\nVerify GPU availability and display hardware specifications for optimal training configuration.","metadata":{}},{"cell_type":"code","source":"import torch\n\n# Verify CUDA availability and display GPU specifications\nprint(f\"CUDA available: {torch.cuda.is_available()}\")\nprint(f\"Number of GPUs: {torch.cuda.device_count()}\")\n\nif torch.cuda.is_available():\n    # Display current GPU details for training optimization\n    print(f\"Current GPU: {torch.cuda.current_device()}\")\n    print(f\"GPU name: {torch.cuda.get_device_name()}\")\n    print(f\"GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\nelse:\n    # Provide guidance for enabling GPU in Colab\n    print(\"‚ö†Ô∏è  No GPU available. This notebook requires a GPU for efficient training.\")\n    print(\"In Colab: Runtime ‚Üí Change runtime type ‚Üí Hardware accelerator ‚Üí GPU\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-25T18:57:11.224702Z","iopub.execute_input":"2025-09-25T18:57:11.224954Z","iopub.status.idle":"2025-09-25T18:57:12.980249Z","shell.execute_reply.started":"2025-09-25T18:57:11.224932Z","shell.execute_reply":"2025-09-25T18:57:12.979626Z"}},"outputs":[{"name":"stdout","text":"CUDA available: True\nNumber of GPUs: 2\nCurrent GPU: 0\nGPU name: Tesla T4\nGPU memory: 15.8 GB\n","output_type":"stream"}],"execution_count":4},{"cell_type":"markdown","source":"## Core Library Imports\nImport essential libraries for pre-training, model configuration, and experiment tracking.","metadata":{}},{"cell_type":"code","source":"# Model and tokenization\nfrom unsloth import FastLanguageModel\n\n# Training and Setup\nfrom unsloth import (\n    UnslothTrainer,\n    UnslothTrainingArguments,\n    is_bfloat16_supported\n)\n\n# Dataset handling\nfrom datasets import load_dataset","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-25T19:01:50.332668Z","iopub.execute_input":"2025-09-25T19:01:50.332951Z","iopub.status.idle":"2025-09-25T19:01:50.337073Z","shell.execute_reply.started":"2025-09-25T19:01:50.332933Z","shell.execute_reply":"2025-09-25T19:01:50.336133Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"model_name = 'unsloth/gemma-3-1b-pt'\nmax_seq_len = 1024\ndtype = torch.float16\nload_in_4bit = True\n\nprint(f'Loading model: {model_name}')\nprint(f'Max input length: {max_seq_len}')\nprint(f'Model dtype: {dtype}')\nprint(f'Is 4bit quantization supported: {load_in_4bit}')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-25T18:57:53.589040Z","iopub.execute_input":"2025-09-25T18:57:53.589323Z","iopub.status.idle":"2025-09-25T18:57:53.593882Z","shell.execute_reply.started":"2025-09-25T18:57:53.589296Z","shell.execute_reply":"2025-09-25T18:57:53.593161Z"}},"outputs":[{"name":"stdout","text":"Loading model: unsloth/gemma-3-1b-pt\nMax input length: 1024\nModel dtype: torch.float16\nIs 4bit quantization supported: True\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"# Load model with automatic device mapping\nmodel, tokenizer = FastLanguageModel.from_pretrained(\n    model_name = model_name,\n    max_seq_length = max_seq_len,\n    dtype = dtype,\n    load_in_4bit = load_in_4bit,\n)\n\n# Ensure tokenizer has proper padding token for batch processing\nif tokenizer.pad_token is None:\n    tokenizer.pad_token = tokenizer.eos_token","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-25T18:58:05.309402Z","iopub.execute_input":"2025-09-25T18:58:05.310121Z","iopub.status.idle":"2025-09-25T18:58:22.760924Z","shell.execute_reply.started":"2025-09-25T18:58:05.310097Z","shell.execute_reply":"2025-09-25T18:58:22.760100Z"}},"outputs":[{"name":"stdout","text":"==((====))==  Unsloth 2025.9.7: Fast Gemma3 patching. Transformers: 4.55.4.\n   \\\\   /|    Tesla T4. Num GPUs = 2. Max memory: 14.741 GB. Platform: Linux.\nO^O/ \\_/ \\    Torch: 2.8.0+cu128. CUDA: 7.5. CUDA Toolkit: 12.8. Triton: 3.4.0\n\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.32.post2. FA2 = False]\n \"-____-\"     Free license: http://github.com/unslothai/unsloth\nUnsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\nUnsloth: Using float16 precision for gemma3 won't work! Using float32.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/1.00G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fdd084eb71a148b1a1e0ba794ec1aad2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/233 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"24cbd9d429bb47fe9888db16fdac2dd1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3a23f1c6b9944e9a835d810c6bb3237b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.model:   0%|          | 0.00/4.69M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"06ab6f2539df42dfbd0ac581ade71ef3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/33.4M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9f0e3c4309d84c2a9d1faf733fa44027"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"added_tokens.json:   0%|          | 0.00/35.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"acad5162f6c8430a93249469ff0a618a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/662 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9bf83bccad4e4814909fcaf886b9cdd1"}},"metadata":{}}],"execution_count":8},{"cell_type":"code","source":"print(f\"‚úÖ Model loaded successfully!\")\nprint(f\"üìä Model parameters: ~{sum(p.numel() for p in model.parameters()) / 1e6:.1f}M\")\nprint(f\"üßÆ Quantized parameters: ~{sum(p.numel() for p in model.parameters() if hasattr(p, 'quant_type')) / 1e6:.1f}M\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-25T18:58:22.762116Z","iopub.execute_input":"2025-09-25T18:58:22.762377Z","iopub.status.idle":"2025-09-25T18:58:22.769792Z","shell.execute_reply.started":"2025-09-25T18:58:22.762343Z","shell.execute_reply":"2025-09-25T18:58:22.769122Z"}},"outputs":[{"name":"stdout","text":"‚úÖ Model loaded successfully!\nüìä Model parameters: ~662.9M\nüßÆ Quantized parameters: ~336.9M\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"def compute_model_size(model):\n    n_params = 0\n    for p in model.parameters():\n        n_params += p.nelement() * p.element_size()\n    for p in model.buffers():\n        n_params += p.nelement() * p.element_size()\n\n    return n_params / (1024 ** 3)\n\nprint(f\"üìä Model size : {compute_model_size(model):.2f} GB\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-25T18:58:55.298156Z","iopub.execute_input":"2025-09-25T18:58:55.298456Z","iopub.status.idle":"2025-09-25T18:58:55.305945Z","shell.execute_reply.started":"2025-09-25T18:58:55.298435Z","shell.execute_reply":"2025-09-25T18:58:55.305045Z"}},"outputs":[{"name":"stdout","text":"üìä Model size : 0.92 GB\n","output_type":"stream"}],"execution_count":13},{"cell_type":"markdown","source":"## Peft Configuration\nConfigure LoRA weight into base model","metadata":{}},{"cell_type":"code","source":"model = FastLanguageModel.get_peft_model(\n    model,\n    r = 64,\n    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n                      \"gate_proj\", \"up_proj\", \"down_proj\",\n                      \"embed_tokens\", \"lm_head\",], \n    lora_alpha = 128,\n    lora_dropout = 0.1,\n    bias = \"none\",    \n    use_gradient_checkpointing = \"unsloth\",\n    use_rslora = True,\n    loftq_config = None,\n)\n\nmodel.print_trainable_parameters()\nprint(f\"üìä Model size : {compute_model_size(model):.2f} GB\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-25T19:00:36.286564Z","iopub.execute_input":"2025-09-25T19:00:36.287409Z","iopub.status.idle":"2025-09-25T19:00:42.179062Z","shell.execute_reply.started":"2025-09-25T19:00:36.287376Z","shell.execute_reply":"2025-09-25T19:00:42.178264Z"}},"outputs":[{"name":"stderr","text":"Unsloth: Dropout = 0 is supported for fast patching. You are using dropout = 0.1.\nUnsloth will patch all other layers, except LoRA matrices, causing a performance hit.\n/usr/local/lib/python3.11/dist-packages/peft/tuners/tuners_utils.py:550: UserWarning: Model with `tie_word_embeddings=True` and the tied_target_modules=['model.embed_tokens', 'lm_head'] are part of the adapter. This can lead to complications, for example when merging the adapter or converting your model to formats other than safetensors. See for example https://github.com/huggingface/peft/issues/2018.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Unsloth: Making `model.base_model.model.model.embed_tokens` require gradients\ntrainable params: 69,033,984 || all params: 1,085,770,880 || trainable%: 6.3581\nüìä Model size : 1.24 GB\n","output_type":"stream"}],"execution_count":14},{"cell_type":"markdown","source":"## Swissprot Dataset Setup\nConfigure the Swissprot sequences dataset.","metadata":{}},{"cell_type":"code","source":"# Define structured output format for protein formatting\nprotein_start = \"<start_protein>\"   # Begin protein sequence\nprotein_end = \"<end_protein>\"       # End protein sequence\neos_token = tokenizer.eos_token     # EOS so that generation does not goes forever","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-25T19:01:14.512731Z","iopub.execute_input":"2025-09-25T19:01:14.513473Z","iopub.status.idle":"2025-09-25T19:01:14.518169Z","shell.execute_reply.started":"2025-09-25T19:01:14.513441Z","shell.execute_reply":"2025-09-25T19:01:14.517414Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"def process_dataset_example(example):\n    \"\"\"Convert Swissprot example to formatted protein\"\"\"\n    sequence = example[\"Sequence\"]\n\n    # Experim: let the tokenizer decide\n    # sequence = ' '.join(list(sequence)) # Amino acid level tokenization\n    text = f'{protein_start} {sequence} {protein_end} {eos_token}'\n    \n    return {\n        \"text\": text,\n    }\n\nprint(\"‚úÖ Dataset processing functions defined\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-25T19:01:15.256493Z","iopub.execute_input":"2025-09-25T19:01:15.257130Z","iopub.status.idle":"2025-09-25T19:01:15.262369Z","shell.execute_reply.started":"2025-09-25T19:01:15.257102Z","shell.execute_reply":"2025-09-25T19:01:15.261312Z"}},"outputs":[{"name":"stdout","text":"‚úÖ Dataset processing functions defined\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"# Load and preprocess Swissprot training dataset\nprint(\"üîÑ Loading Swissprot sequences dataset...\")\ndataset = load_dataset(\"khairi/uniprot-swissprot\")\n\n# Apply conversation formatting to all examples\ndataset = dataset.map(process_dataset_example) \\\n\ntrain_data = dataset['train']\nvalid_data = dataset['validation'].select(range(128)) # Pick 128 protein for evaluation\n\nprint(f\"‚úÖ Dataset loaded and processed!\")\nprint(f\"üìä Training examples: {len(train_data):,}\")\nprint(f\"üìä Validation examples: {len(valid_data):,}\")\nprint(f\"üéØ Sample protein: {train_data[0]['text']}\")\nprint(f\"üéØ Sample protein (tokenized): {' '.join(tokenizer.convert_ids_to_tokens(tokenizer.encode(train_data[0]['text'])))}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-25T19:35:28.132441Z","iopub.execute_input":"2025-09-25T19:35:28.133339Z","iopub.status.idle":"2025-09-25T19:35:28.705698Z","shell.execute_reply.started":"2025-09-25T19:35:28.133301Z","shell.execute_reply":"2025-09-25T19:35:28.704925Z"}},"outputs":[{"name":"stdout","text":"üîÑ Loading Swissprot sequences dataset...\n‚úÖ Dataset loaded and processed!\nüìä Training examples: 455,692\nüìä Validation examples: 128\nüéØ Sample protein: <start_protein> MSLEQKKGADIISKILQIQNSIGKTTSPSTLKTKLSEISRKEQENARIQSKLSDLQKKKIDIDNKLLKEKQNLIKEEILERKKLEVLTKKQQKDEIEHQKKLKREIDAIKASTQYITDVSISSYNNTIPETEPEYDLFISHASEDKEDFVRPLAETLQQLGVNVWYDEFTLKVGDSLRQKIDSGLRNSKYGTVVLSTDFIKKDWTNYELDGLVAREMNGHKMILPIWHKITKNDVLDYSPNLADKVALNTSVNSIEEIAHQLADVILNR <end_protein> <eos>\nüéØ Sample protein (tokenized): <bos> < start _ protein > ‚ñÅMS LE Q KK G ADI ISK IL Q IQ NS IG KT TSP STL KT KL SE ISR KE Q EN ARI Q SK L SDL Q KK K ID ID NK LL KE KQ NL IK EE ILER KK LEV LT KK QQ K DE IE HQ K KL K RE ID AI K AST Q Y IT D VS ISS Y NN TIP ETE PE Y DL FISH ASE DK ED F VR PLA ET LQ QL GV NV WY DE FT L KV G DS LR QK IDS GL R NS KY G TV VL ST DF IK KD WT NY ELD GL VA REM NG HK MIL PI WH KIT K ND V LD Y SP NL AD K VAL N TS VN SI EE IA H QL ADV IL NR ‚ñÅ< end _ protein > ‚ñÅ <eos>\n","output_type":"stream"}],"execution_count":56},{"cell_type":"markdown","source":"## Training Setup\nConfigure training parameters optimized for learning a new language with memory constraints.","metadata":{}},{"cell_type":"code","source":"training_args = UnslothTrainingArguments(\n    per_device_train_batch_size = 4,\n    gradient_accumulation_steps = 8,\n\n    # Use warmup_ratio and num_train_epochs for longer runs!\n    # max_steps = 120,\n    # warmup_steps = 10,\n    warmup_ratio = 0.1,\n    num_train_epochs = 1,\n\n    learning_rate = 5e-5,\n    embedding_learning_rate = 2e-5,\n\n    fp16 = not is_bfloat16_supported(),\n    bf16 = is_bfloat16_supported(),\n    logging_steps = 400,\n    eval_steps = 400,\n    save_steps = 400,\n    eval_strategy = 'steps',\n    save_total_limit = 3,\n    load_best_model_at_end = True,\n    optim = \"adamw_8bit\",\n    weight_decay = 0.01,\n    lr_scheduler_type = \"cosine\",\n    \n    output_dir = \"/tmp/outputs\",\n    run_name = 'gemma2-cpt-swissport',\n    report_to = \"wandb\", # Use this for WandB etc\n\n    # Push to Hub, set true in production\n    push_to_hub=True,\n    hub_model_id='khairi/Hisoka-1B'\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-25T19:35:37.511080Z","iopub.execute_input":"2025-09-25T19:35:37.511998Z","iopub.status.idle":"2025-09-25T19:35:37.565700Z","shell.execute_reply.started":"2025-09-25T19:35:37.511966Z","shell.execute_reply":"2025-09-25T19:35:37.564959Z"}},"outputs":[],"execution_count":57},{"cell_type":"code","source":"trainer = UnslothTrainer(\n    model = model,\n    tokenizer = tokenizer,\n    train_dataset = train_data,\n    eval_dataset = valid_data,\n    dataset_text_field = \"text\",\n    max_seq_length = max_seq_len,\n    dataset_num_proc = 2,\n    args = training_args,\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-25T19:35:49.212674Z","iopub.execute_input":"2025-09-25T19:35:49.212972Z","iopub.status.idle":"2025-09-25T19:35:51.618798Z","shell.execute_reply.started":"2025-09-25T19:35:49.212951Z","shell.execute_reply":"2025-09-25T19:35:51.617993Z"}},"outputs":[{"name":"stdout","text":"Unsloth: Switching to float32 training since model cannot work with float16\n","output_type":"stream"}],"execution_count":58},{"cell_type":"code","source":"# Execute CPT\nprint(\"üöÄ Starting CPT...\")\n\n# Run the training process\ntrainer.train()\n\nprint(\"‚úÖ Training completed successfully!\")\nprint(f\"üíæ Model saved to: {training_args.output_dir}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-25T19:35:59.770948Z","iopub.execute_input":"2025-09-25T19:35:59.771256Z"}},"outputs":[{"name":"stderr","text":"==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n   \\\\   /|    Num examples = 455,692 | Num Epochs = 1 | Total steps = 14,241\nO^O/ \\_/ \\    Batch size per device = 4 | Gradient accumulation steps = 8\n\\        /    Data Parallel GPUs = 1 | Total batch size (4 x 8 x 1) = 32\n \"-____-\"     Trainable parameters = 69,033,984 of 1,085,770,880 (6.36% trained)\n","output_type":"stream"},{"name":"stdout","text":"üöÄ Starting CPT...\n","output_type":"stream"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mflursky\u001b[0m to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.20.1"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250925_193609-aemskdpx</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/flursky/Unsloth-CPT/runs/aemskdpx' target=\"_blank\">gemma2-cpt-swissport</a></strong> to <a href='https://wandb.ai/flursky/Unsloth-CPT' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/flursky/Unsloth-CPT' target=\"_blank\">https://wandb.ai/flursky/Unsloth-CPT</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/flursky/Unsloth-CPT/runs/aemskdpx' target=\"_blank\">https://wandb.ai/flursky/Unsloth-CPT/runs/aemskdpx</a>"},"metadata":{}},{"name":"stdout","text":"Unsloth: Will smartly offload gradients to save VRAM!\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='63' max='14241' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [   63/14241 05:32 < 21:29:07, 0.18 it/s, Epoch 0.00/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"},"metadata":{}}],"execution_count":null},{"cell_type":"code","source":"def generate_protein():\n    inputs = tokenizer(f\"{protein_start}\", return_tensors='pt')\n\n    inputs = {k: v.to(model.device) for k, v in inputs.items()}\n    output = model.generate(**inputs, max_new_tokens=512, top_k=250, do_sample=True)\n    print(tokenizer.decode(output[0]))\n\ngenerate_protein()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def causal_lm():\n    import random\n    prompt = random.choice([\n        'Hello World!',\n        'Once upon a time',\n        'BRAC5 is',\n    ])\n    print(f\"prompt >> {prompt}\")\n    inputs = tokenizer(prompt, return_tensors='pt')\n\n    inputs = {k: v.to(model.device) for k, v in inputs.items()}\n    output = model.generate(**inputs, max_new_tokens=256, top_k=250, do_sample=True)\n    print(tokenizer.decode(output[0]))\n\ncausal_lm()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}