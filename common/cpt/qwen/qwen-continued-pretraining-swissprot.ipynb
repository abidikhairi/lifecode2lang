{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Continued Pretraining of Qwen-0.5B On Swissprot Sequences\n\nAuthor: [Khairi Abidi](https://github.com/abidikhairi/)\n\nThis notebook demonstrates continued pretraining for protein sequence modeling.\n\nKey Features:\n\n- Memory Efficient: LoRA for consumer GPUs\n\nThe model learns to generate model/functional protein sequences.\n\n## Installation and Setup\nInstall the required packages for continued pretraining with memory-efficient techniques.","metadata":{}},{"cell_type":"code","source":"%env CUDA_VISIBLE_DEVICES=0,1","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-24T21:46:07.633286Z","iopub.execute_input":"2025-09-24T21:46:07.633621Z","iopub.status.idle":"2025-09-24T21:46:07.638727Z","shell.execute_reply.started":"2025-09-24T21:46:07.633575Z","shell.execute_reply":"2025-09-24T21:46:07.637823Z"}},"outputs":[{"name":"stdout","text":"env: CUDA_VISIBLE_DEVICES=0,1\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"%%capture\n!pip install --quiet transformers datasets trl bitsandbytes peft trackio","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-24T21:46:09.063815Z","iopub.execute_input":"2025-09-24T21:46:09.064094Z","iopub.status.idle":"2025-09-24T21:46:13.016035Z","shell.execute_reply.started":"2025-09-24T21:46:09.064072Z","shell.execute_reply":"2025-09-24T21:46:13.014962Z"}},"outputs":[],"execution_count":10},{"cell_type":"markdown","source":"## Connect to 3rd party services\n\n- **WandB**: for experiment tracking.\n- **HuggingFace Hub**: for model checkpoints uploading.","metadata":{}},{"cell_type":"code","source":"from kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\nhf_token = user_secrets.get_secret(\"HUGGING_FACE_TOKEN\")\nwandb_token = user_secrets.get_secret(\"WANDB_API_KEY\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-24T21:46:23.459225Z","iopub.execute_input":"2025-09-24T21:46:23.459609Z","iopub.status.idle":"2025-09-24T21:46:23.606781Z","shell.execute_reply.started":"2025-09-24T21:46:23.459561Z","shell.execute_reply":"2025-09-24T21:46:23.605951Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"%env WANDB_PROJECT=Qwen-CPT","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-24T21:46:24.267361Z","iopub.execute_input":"2025-09-24T21:46:24.267716Z","iopub.status.idle":"2025-09-24T21:46:24.272742Z","shell.execute_reply.started":"2025-09-24T21:46:24.267692Z","shell.execute_reply":"2025-09-24T21:46:24.271899Z"}},"outputs":[{"name":"stdout","text":"env: WANDB_PROJECT=Qwen-CPT\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"!wandb login {wandb_token}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-24T21:46:27.933073Z","iopub.execute_input":"2025-09-24T21:46:27.933402Z","iopub.status.idle":"2025-09-24T21:46:29.652746Z","shell.execute_reply.started":"2025-09-24T21:46:27.933379Z","shell.execute_reply":"2025-09-24T21:46:29.651709Z"}},"outputs":[{"name":"stdout","text":"\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n\u001b[34m\u001b[1mwandb\u001b[0m: W&B API key is configured. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"!huggingface-cli login --token {hf_token}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-24T21:46:29.794346Z","iopub.execute_input":"2025-09-24T21:46:29.794682Z","iopub.status.idle":"2025-09-24T21:46:30.431669Z","shell.execute_reply.started":"2025-09-24T21:46:29.794651Z","shell.execute_reply":"2025-09-24T21:46:30.430819Z"}},"outputs":[{"name":"stdout","text":"\u001b[33m⚠️  Warning: 'huggingface-cli login' is deprecated. Use 'hf auth login' instead.\u001b[0m\nThe token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `hf`CLI if you want to set the git credential as well.\nToken is valid (permission: write).\nThe token `KAGGLE_TOKEN` has been saved to /root/.cache/huggingface/stored_tokens\nYour token has been saved to /root/.cache/huggingface/token\nLogin successful.\nThe current active token is: `KAGGLE_TOKEN`\n","output_type":"stream"}],"execution_count":14},{"cell_type":"markdown","source":"## GPU Environment Detection\nVerify GPU availability and display hardware specifications for optimal training configuration.","metadata":{}},{"cell_type":"code","source":"import torch\n\n# Verify CUDA availability and display GPU specifications\nprint(f\"CUDA available: {torch.cuda.is_available()}\")\nprint(f\"Number of GPUs: {torch.cuda.device_count()}\")\n\nif torch.cuda.is_available():\n    # Display current GPU details for training optimization\n    print(f\"Current GPU: {torch.cuda.current_device()}\")\n    print(f\"GPU name: {torch.cuda.get_device_name()}\")\n    print(f\"GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\nelse:\n    # Provide guidance for enabling GPU in Colab\n    print(\"⚠️  No GPU available. This notebook requires a GPU for efficient training.\")\n    print(\"In Colab: Runtime → Change runtime type → Hardware accelerator → GPU\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-24T21:46:32.263636Z","iopub.execute_input":"2025-09-24T21:46:32.263951Z","iopub.status.idle":"2025-09-24T21:46:32.270501Z","shell.execute_reply.started":"2025-09-24T21:46:32.263923Z","shell.execute_reply":"2025-09-24T21:46:32.269542Z"}},"outputs":[{"name":"stdout","text":"CUDA available: True\nNumber of GPUs: 2\nCurrent GPU: 0\nGPU name: Tesla T4\nGPU memory: 15.8 GB\n","output_type":"stream"}],"execution_count":15},{"cell_type":"markdown","source":"## Core Library Imports\nImport essential libraries for pre-training, model configuration, and experiment tracking.","metadata":{}},{"cell_type":"code","source":"# Model and tokenization\nfrom transformers import (\n    AutoModelForCausalLM,            # Causal language model loading\n    AutoTokenizer,                   # Text tokenization\n    DataCollatorForLanguageModeling, # Batch inputs handling\n    BitsAndBytesConfig,              # Quantization configuration\n)\n\n# Model optimization\nfrom torch.optim import AdamW\nfrom transformers import get_scheduler\n\n# Training and Setup\nfrom transformers import (\n    Trainer,\n    TrainingArguments\n)\n\n# Dataset handling\nfrom datasets import load_dataset\n\n# Logging configuration\nimport logging\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-24T21:46:34.877139Z","iopub.execute_input":"2025-09-24T21:46:34.877926Z","iopub.status.idle":"2025-09-24T21:46:34.882957Z","shell.execute_reply.started":"2025-09-24T21:46:34.877898Z","shell.execute_reply":"2025-09-24T21:46:34.882085Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"# Select model optimized for instruction-following and reasoning\nmodel_name = \"Qwen/Qwen2.5-0.5B\"          # 0.5B parameter model balances capability and memory usage\nmax_seq_length = 768                      # Token limit for protein sequences (reduce if OOM)\n\nprint(f\"Loading model: {model_name}\")\nprint(f\"Max sequence length: {max_seq_length}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-24T21:46:36.720936Z","iopub.execute_input":"2025-09-24T21:46:36.721228Z","iopub.status.idle":"2025-09-24T21:46:36.725922Z","shell.execute_reply.started":"2025-09-24T21:46:36.721206Z","shell.execute_reply":"2025-09-24T21:46:36.725104Z"}},"outputs":[{"name":"stdout","text":"Loading model: Qwen/Qwen2.5-0.5B\nMax sequence length: 768\n","output_type":"stream"}],"execution_count":17},{"cell_type":"code","source":"# Load model with automatic device mapping\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    device_map=\"auto\",                    # Auto-distribute across available GPUs/CPU\n    trust_remote_code=True,               # Allow custom model code execution\n    dtype=torch.float16,                 # Use FP16 for non-quantized operations\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-24T21:46:37.067818Z","iopub.execute_input":"2025-09-24T21:46:37.068093Z","iopub.status.idle":"2025-09-24T21:46:43.673311Z","shell.execute_reply.started":"2025-09-24T21:46:37.068071Z","shell.execute_reply":"2025-09-24T21:46:43.672324Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/681 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9b7b36329f7c441d9e930e2dae58db2d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/988M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1883202c739c460fadd132424357ce4a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/138 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"989f90647b22459cb3af65b8999b88bc"}},"metadata":{}}],"execution_count":18},{"cell_type":"code","source":"# Load corresponding tokenizer\ntokenizer = AutoTokenizer.from_pretrained(\n    model_name,\n    trust_remote_code=True               # Allow custom tokenizer code\n)\n\n# Ensure tokenizer has proper padding token for batch processing\nif tokenizer.pad_token is None:\n    tokenizer.pad_token = tokenizer.eos_token","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-24T21:46:43.674768Z","iopub.execute_input":"2025-09-24T21:46:43.675047Z","iopub.status.idle":"2025-09-24T21:46:44.926576Z","shell.execute_reply.started":"2025-09-24T21:46:43.675022Z","shell.execute_reply":"2025-09-24T21:46:44.925717Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"acf23e2cc474474c8b53e2342bfee24f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"81aa795b34a14c2784fe1b5bb97ffb34"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0952b6ad56064b26aca5f2cac31aabd5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1c5714d9029d48bfab9fc95f31e3954d"}},"metadata":{}}],"execution_count":19},{"cell_type":"code","source":"print(f\"✅ Model loaded successfully!\")\nprint(f\"📊 Model parameters: ~{sum(p.numel() for p in model.parameters()) / 1e6:.1f}M\")\nprint(f\"🧮 Quantized parameters: ~{sum(p.numel() for p in model.parameters() if hasattr(p, 'quant_type')) / 1e6:.1f}M\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-24T21:46:44.927406Z","iopub.execute_input":"2025-09-24T21:46:44.927684Z","iopub.status.idle":"2025-09-24T21:46:44.934143Z","shell.execute_reply.started":"2025-09-24T21:46:44.927657Z","shell.execute_reply":"2025-09-24T21:46:44.933182Z"}},"outputs":[{"name":"stdout","text":"✅ Model loaded successfully!\n📊 Model parameters: ~494.0M\n🧮 Quantized parameters: ~0.0M\n","output_type":"stream"}],"execution_count":20},{"cell_type":"code","source":"def compute_model_size(model):\n    n_params = 0\n    for p in model.parameters():\n        n_params += p.nelement() * p.element_size()\n    for p in model.buffers():\n        n_params += p.nelement() * p.element_size()\n\n    return n_params / (1024 ** 3)\n\nprint(f\"📊 Model size : {compute_model_size(model):.2f} GB\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-24T21:46:44.935550Z","iopub.execute_input":"2025-09-24T21:46:44.935865Z","iopub.status.idle":"2025-09-24T21:46:45.177290Z","shell.execute_reply.started":"2025-09-24T21:46:44.935842Z","shell.execute_reply":"2025-09-24T21:46:45.176520Z"}},"outputs":[{"name":"stdout","text":"📊 Model size : 0.92 GB\n","output_type":"stream"}],"execution_count":21},{"cell_type":"markdown","source":"## Swissprot Dataset Setup\nConfigure the Swissprot sequences dataset.","metadata":{}},{"cell_type":"code","source":"# Define structured output format for protein formatting\nprotein_start = \"<start_protein>\"   # Begin protein sequence\nprotein_end = \"<end_protein>\"       # End protein sequence\neos_token = tokenizer.eos_token     # EOS so that generation does not goes forever","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-24T21:46:57.823876Z","iopub.execute_input":"2025-09-24T21:46:57.824204Z","iopub.status.idle":"2025-09-24T21:46:57.828448Z","shell.execute_reply.started":"2025-09-24T21:46:57.824181Z","shell.execute_reply":"2025-09-24T21:46:57.827649Z"}},"outputs":[],"execution_count":22},{"cell_type":"code","source":"def process_dataset_example(example):\n    \"\"\"Convert Swissprot example to formatted protein\"\"\"\n    sequence = example[\"Sequence\"]\n\n    # Experim: let the tokenizer decide\n    sequence = ' '.join(list(sequence)) # Amino acid level tokenization\n    text = f'{protein_start} {sequence} {protein_end} {eos_token}'\n    \n    return {\n        \"text\": text,\n    }\n\nprint(\"✅ Dataset processing functions defined\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-24T21:47:00.326261Z","iopub.execute_input":"2025-09-24T21:47:00.326539Z","iopub.status.idle":"2025-09-24T21:47:00.332000Z","shell.execute_reply.started":"2025-09-24T21:47:00.326518Z","shell.execute_reply":"2025-09-24T21:47:00.331094Z"}},"outputs":[{"name":"stdout","text":"✅ Dataset processing functions defined\n","output_type":"stream"}],"execution_count":23},{"cell_type":"code","source":"def tokenize_dataset_example(examples):\n    return tokenizer(examples['text'], return_tensors='pt', padding=True)\n\nprint(\"✅ Dataset tokenization functions defined (batch mode)\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-24T21:47:02.735794Z","iopub.execute_input":"2025-09-24T21:47:02.736401Z","iopub.status.idle":"2025-09-24T21:47:02.741313Z","shell.execute_reply.started":"2025-09-24T21:47:02.736376Z","shell.execute_reply":"2025-09-24T21:47:02.740377Z"}},"outputs":[{"name":"stdout","text":"✅ Dataset tokenization functions defined (batch mode)\n","output_type":"stream"}],"execution_count":24},{"cell_type":"code","source":"# Load and preprocess Swissprot training dataset\nprint(\"🔄 Loading Swissprot sequences dataset...\")\ndataset = load_dataset(\"khairi/uniprot-swissprot\")\n\n# Apply conversation formatting to all examples\ndataset = dataset.map(process_dataset_example) \\\n    .map(tokenize_dataset_example, batched=True, batch_size=32)\n\ntrain_data = dataset['train']\nvalid_data = dataset['validation'].select(range(128)) # Pick 128 protein for evaluation\n\nprint(f\"✅ Dataset loaded and processed!\")\nprint(f\"📊 Training examples: {len(train_data):,}\")\nprint(f\"📊 Validation examples: {len(valid_data):,}\")\nprint(f\"🎯 Sample protein: {train_data[0]['text']}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-24T21:47:04.688346Z","iopub.execute_input":"2025-09-24T21:47:04.688681Z","iopub.status.idle":"2025-09-24T21:51:55.462802Z","shell.execute_reply.started":"2025-09-24T21:47:04.688656Z","shell.execute_reply":"2025-09-24T21:51:55.462086Z"}},"outputs":[{"name":"stdout","text":"🔄 Loading Swissprot sequences dataset...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/169 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5d21f854ecb74ed89036d2e99e75ca18"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train.parquet:   0%|          | 0.00/116M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"639fe2e41b684c759acec4da755093cd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"valid.parquet:   0%|          | 0.00/540k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"283cd260816343248f8ab05d10466d24"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"test.parquet:   0%|          | 0.00/2.66M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"afee652430fe4a77aa9c8bf431736bd2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/455692 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"279f53175a53463b9e0abdb8d3d6621c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split:   0%|          | 0/2000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"30dd1067f87b47b18e8f69186f74b320"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/10000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"068279967d474e0e8bda0a998eca1445"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/455692 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"61e22db3a5834a28ad4b443f87ddfecc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/2000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d4a58eb053ed4a0bbcdb28c856386504"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/10000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f508f4271f414bcebef4bb68dc10c1dd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/455692 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f0d9b21cd13a4890a95f3313623daa9c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/2000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"195798d199a74b16985c5454f10fa35c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/10000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"83f9377c474d41b985da1c44254461cc"}},"metadata":{}},{"name":"stdout","text":"✅ Dataset loaded and processed!\n📊 Training examples: 455,692\n📊 Validation examples: 128\n🎯 Sample protein: <start_protein> M S L E Q K K G A D I I S K I L Q I Q N S I G K T T S P S T L K T K L S E I S R K E Q E N A R I Q S K L S D L Q K K K I D I D N K L L K E K Q N L I K E E I L E R K K L E V L T K K Q Q K D E I E H Q K K L K R E I D A I K A S T Q Y I T D V S I S S Y N N T I P E T E P E Y D L F I S H A S E D K E D F V R P L A E T L Q Q L G V N V W Y D E F T L K V G D S L R Q K I D S G L R N S K Y G T V V L S T D F I K K D W T N Y E L D G L V A R E M N G H K M I L P I W H K I T K N D V L D Y S P N L A D K V A L N T S V N S I E E I A H Q L A D V I L N R <end_protein> <|endoftext|>\n","output_type":"stream"}],"execution_count":25},{"cell_type":"markdown","source":"## Training Setup\nConfigure training parameters optimized for learning a new language with memory constraints.","metadata":{}},{"cell_type":"code","source":"# Prepare data collator\ndata_collator = DataCollatorForLanguageModeling(mlm=False, tokenizer=tokenizer)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-24T21:52:50.092980Z","iopub.execute_input":"2025-09-24T21:52:50.093399Z","iopub.status.idle":"2025-09-24T21:52:50.097410Z","shell.execute_reply.started":"2025-09-24T21:52:50.093375Z","shell.execute_reply":"2025-09-24T21:52:50.096690Z"}},"outputs":[],"execution_count":26},{"cell_type":"code","source":"# Configure CPT training parameters for sequence modeling\ntraining_args = TrainingArguments(\n    # Memory-efficient batch configuration\n    per_device_train_batch_size=4,   # Small batch for GPU memory constraints\n    gradient_accumulation_steps=8,   # Effective batch size = 8 * 4 = 32\n       \n    # Training duration and monitoring\n    max_steps=1000,                    # Short demo run (increase to 500+ for production)\n    logging_steps=25,                  # Log metrics every step for close monitoring\n    save_steps=25,\n    eval_steps=25,\n    eval_strategy='steps',\n    \n    # Stability and output configuration\n    output_dir=\"./cpt_outputs\",\n    max_grad_norm=0.1,               # Aggressive gradient clipping for stable training\n    report_to=\"wandb\",                # use wandb for experiment tracking\n    run_name='qwen0.5B-cpt-swissprot',\n    fp16=True,\n\n    # Push to Hub, uncomment in production\n    push_to_hub=True,\n    hub_model_id='khairi/Shizuku-0.5B'\n    \n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-24T21:53:14.409088Z","iopub.execute_input":"2025-09-24T21:53:14.409387Z","iopub.status.idle":"2025-09-24T21:53:14.459890Z","shell.execute_reply.started":"2025-09-24T21:53:14.409368Z","shell.execute_reply":"2025-09-24T21:53:14.459099Z"}},"outputs":[],"execution_count":27},{"cell_type":"markdown","source":"### Optimizer and Scheduler Setup\n\nConfigure **AdamW optimizer** with different learning rates for the embedding and layer parameters of the model.  \nThen sets up a **cosine learning rate scheduler** with a short warmup (fast warmup/slow cooldown) and total training steps.","metadata":{}},{"cell_type":"code","source":"print(\"🚀 Initializing optimizer...\")\nprint(\"🌙 Setting up cosine LR scheduler...\")\n\noptimizer = AdamW([\n        {'params': model.model.embed_tokens.parameters(), 'lr': 1e-4},\n        {'params': model.model.layers.parameters(), 'lr': 3e-4}\n    ],\n    betas=(0.99, 0.98),\n    weight_decay=0.01\n)\n\nlr_scheduler = get_scheduler(\n    name='cosine',\n    optimizer=optimizer,\n    num_warmup_steps=50,\n    num_training_steps=1000\n)\n\nprint(\"✅ Optimizer ready!\")\nprint(\"✨ LR scheduler ready!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-24T21:53:27.332975Z","iopub.execute_input":"2025-09-24T21:53:27.333843Z","iopub.status.idle":"2025-09-24T21:53:27.343037Z","shell.execute_reply.started":"2025-09-24T21:53:27.333806Z","shell.execute_reply":"2025-09-24T21:53:27.342087Z"}},"outputs":[{"name":"stdout","text":"🚀 Initializing optimizer...\n🌙 Setting up cosine LR scheduler...\n✅ Optimizer ready!\n✨ LR scheduler ready!\n","output_type":"stream"}],"execution_count":29},{"cell_type":"code","source":"trainer = Trainer(\n    model=model,                           #  Qwen0.5B model\n    train_dataset=train_data,              # Training dataset\n    eval_dataset=valid_data,               # Evaluation dataset\n    args=training_args,                    # Training configuration\n    data_collator=data_collator,           # Batch handling\n    optimizers=(optimizer, lr_scheduler)   # Optimization\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-24T21:53:29.931255Z","iopub.execute_input":"2025-09-24T21:53:29.932166Z","iopub.status.idle":"2025-09-24T21:53:31.996989Z","shell.execute_reply.started":"2025-09-24T21:53:29.932132Z","shell.execute_reply":"2025-09-24T21:53:31.996031Z"}},"outputs":[],"execution_count":30},{"cell_type":"code","source":"# Execute CPT\nprint(\"🚀 Starting CPT...\")\n\n# Run the training process\ntrainer.train()\n\nprint(\"✅ Training completed successfully!\")\nprint(f\"💾 Model saved to: {training_args.output_dir}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-24T21:53:36.796764Z","iopub.execute_input":"2025-09-24T21:53:36.797583Z","iopub.status.idle":"2025-09-24T21:53:55.348946Z","shell.execute_reply.started":"2025-09-24T21:53:36.797554Z","shell.execute_reply":"2025-09-24T21:53:55.347642Z"}},"outputs":[{"name":"stdout","text":"🚀 Starting CPT...\n","output_type":"stream"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mflursky\u001b[0m to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.20.1"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250924_215343-y2oylsxj</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/flursky/Qwen-CPT/runs/y2oylsxj' target=\"_blank\">qwen0.5B-cpt-swissprot</a></strong> to <a href='https://wandb.ai/flursky/Qwen-CPT' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/flursky/Qwen-CPT' target=\"_blank\">https://wandb.ai/flursky/Qwen-CPT</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/flursky/Qwen-CPT/runs/y2oylsxj' target=\"_blank\">https://wandb.ai/flursky/Qwen-CPT/runs/y2oylsxj</a>"},"metadata":{}},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_36/727706048.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Run the training process\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"✅ Training completed successfully!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2317\u001b[0m                 \u001b[0;31m# Disable progress bars when uploading models during checkpoints to avoid polluting stdout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2318\u001b[0m                 \u001b[0mhf_hub_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisable_progress_bars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2319\u001b[0;31m                 return inner_training_loop(\n\u001b[0m\u001b[1;32m   2320\u001b[0m                     \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2321\u001b[0m                     \u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2711\u001b[0m                                     \u001b[0mgrad_norm_context\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimplicit_replication\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2712\u001b[0m                                 \u001b[0;32mwith\u001b[0m \u001b[0mgrad_norm_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2713\u001b[0;31m                                     _grad_norm = self.accelerator.clip_grad_norm_(\n\u001b[0m\u001b[1;32m   2714\u001b[0m                                         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2715\u001b[0m                                         \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_grad_norm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/accelerate/accelerator.py\u001b[0m in \u001b[0;36mclip_grad_norm_\u001b[0;34m(self, parameters, max_norm, norm_type)\u001b[0m\n\u001b[1;32m   2707\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mparameters\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mp\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2708\u001b[0m                         \u001b[0;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2709\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munscale_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2710\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnorm_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2711\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/accelerate/accelerator.py\u001b[0m in \u001b[0;36munscale_gradients\u001b[0;34m(self, optimizer)\u001b[0m\n\u001b[1;32m   2645\u001b[0m                 \u001b[0;32mwhile\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAcceleratedOptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2646\u001b[0m                     \u001b[0mopt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2647\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munscale_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2648\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2649\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclip_grad_norm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/amp/grad_scaler.py\u001b[0m in \u001b[0;36munscale_\u001b[0;34m(self, optimizer)\u001b[0m\n\u001b[1;32m    336\u001b[0m         \u001b[0mfound_inf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfull\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_scale\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    337\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 338\u001b[0;31m         optimizer_state[\"found_inf_per_device\"] = self._unscale_grads_(\n\u001b[0m\u001b[1;32m    339\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minv_scale\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfound_inf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    340\u001b[0m         )\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/amp/grad_scaler.py\u001b[0m in \u001b[0;36m_unscale_grads_\u001b[0;34m(self, optimizer, inv_scale, found_inf, allow_fp16)\u001b[0m\n\u001b[1;32m    258\u001b[0m                         \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mnot\u001b[0m \u001b[0mallow_fp16\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat16\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 260\u001b[0;31m                         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Attempting to unscale FP16 gradients.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    261\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_sparse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m                         \u001b[0;31m# is_coalesced() == False means the sparse grad has values with duplicate indices.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: Attempting to unscale FP16 gradients."],"ename":"ValueError","evalue":"Attempting to unscale FP16 gradients.","output_type":"error"}],"execution_count":31},{"cell_type":"code","source":"with torch.no_grad():\n    print(tokenizer.decode(model.generate(tokenizer.encode(\"Hello World!\", return_tensors=\"pt\").to(\"cuda:0\"))[0]))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"with torch.no_grad():\n    print(tokenizer.decode(model.generate(tokenizer.encode(f\"{protein_start} \", return_tensors=\"pt\").to(\"cuda:0\"))[0], max_new_tokens=16, top_k=250, do_sample=True))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}